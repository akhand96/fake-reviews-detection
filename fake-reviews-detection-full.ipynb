{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef21da0",
   "metadata": {},
   "source": [
    "### This Python Notebook is part of Post Graduation Research, which leverages machine learning algorithms and natural language processing techniques to detect fake reviews, ensuring authenticity in user-generated content.\n",
    "\n",
    "#### Note: 1. Trained and Tested on TensorFlow version: 2.12.0 and Keras version: 2.12.0 \n",
    "#### Note: 2. Dataset is uploaded in Git Repository and can be downloaded from (https://osf.io/3vds7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe186b",
   "metadata": {},
   "source": [
    "## Importing required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8109de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical and data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK libraries for text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sklearn libraries for text feature extraction and machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Regular expression and string manipulation\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Initialize stop words\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b88d46",
   "metadata": {},
   "source": [
    "### Loading dataset and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef59a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reviews-dataset.csv\")\n",
    "\n",
    "df[\"text_\"] = df[\"text_\"].apply(lambda x: x.lower()) #lowercase\n",
    "data = df[[\"text_\",\"label\"]]\n",
    "data[\"label\"] = data[\"label\"].apply(lambda x: 1 if x==\"CG\" else 0) #label_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d3ecb",
   "metadata": {},
   "source": [
    "### Train-Test Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text_\"], data[\"label\"], test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea8bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train),len(y_train))\n",
    "print(len(X_test),len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e738ba",
   "metadata": {},
   "source": [
    "### tf_idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef739d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "# applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(X_train)\n",
    "\n",
    "X_train_tf = tf_idf.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5604e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming test data into tf-idf matrix\n",
    "X_test_tf = tf_idf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1004f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d1dd4",
   "metadata": {},
   "source": [
    "## Analysis using Naive Bayes (NB) Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efcad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted y\n",
    "y_pred = naive_bayes_classifier.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d07b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix (for NB):\\n\", metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b679e26",
   "metadata": {},
   "source": [
    "# Analysis using Support Vector Machine (SVM):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb88656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "\n",
    "clf = SVC(kernel='linear') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35810edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2160b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred, target_names=['Positive', 'Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix (for SVM):\\n\", metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92036b60",
   "metadata": {},
   "source": [
    "# Analysis using Long Short-term Memory (LSTM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "counter = Counter(\" \".join(X_train).split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "myTokenizer = Tokenizer(num_words=300)\n",
    "myTokenizer.fit_on_texts(X_train)\n",
    "print(myTokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb14d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = myTokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = myTokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the maximum number of words per document (for both training and testing)\n",
    "max_words = 300\n",
    "\n",
    "# Pad sequences in X_train and X_test\n",
    "X_train_fin = pad_sequences(X_train_seq, maxlen=max_words)\n",
    "X_test_fin = pad_sequences(X_test_seq, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "vocabulary_size = len(counter.keys())\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocabulary_size, 32, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf987da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83966b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "history = model.fit(X_train_fin, y_train, validation_data = [X_test_fin, y_test],use_multiprocessing=True ,epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).to_csv(\"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0256782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "Fig, ax =plt.subplots(1,2,figsize=(10,3))\n",
    "\n",
    "# summarize history for accuracy\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_title('model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend(['train', 'test'], loc='upper left')\n",
    "# summarize history for loss\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('model loss')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"lstm\",dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d26013",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f030b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, [round(i[0]) for i in pred], target_names=['fake', 'correct']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix (for LSTM)\\n:\", metrics.confusion_matrix(y_test, [round(i[0]) for i in pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4608d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", (6174+5992)/(6174+5992+467+710)) # change values accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0548b86",
   "metadata": {},
   "source": [
    "# Analysis using Multilayer Perceptron (MLP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdcaa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=X_train_tf.shape[1], activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c036831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "history = model.fit(X_train_tf.toarray(), y_train, validation_data = [X_test_tf.toarray(), y_test],use_multiprocessing=True ,epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).to_csv(\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b1c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "Fig, ax =plt.subplots(1,2,figsize=(10,3))\n",
    "\n",
    "# summarize history for accuracy\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_title('model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend(['train', 'test'], loc='upper left')\n",
    "# summarize history for loss\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('model loss')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"mlp\",dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b06068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting \n",
    "pred = model.predict(X_test_tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a157946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, [round(i[0]) for i in pred], target_names=['fake', 'correct']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da901ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix (for MLP):\\n\", metrics.confusion_matrix(y_test, [round(i[0]) for i in pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3025a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", (6065+5955) /(6065+5955+686 + 637)) # change values accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1102057d",
   "metadata": {},
   "source": [
    "# Hybrid of LSTM & MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3451b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining MLP\n",
    "\n",
    "def create_mlp(regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=X_train_tf.shape[1], activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    #model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "    # return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp=create_mlp()\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9966c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "vocabulary_size = len(counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5648fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining LSTM\n",
    "\n",
    "def lstm():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 32, input_length=max_words))\n",
    "    model.add(LSTM(100))\n",
    "    #model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm =lstm()\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "# Concatenating LSTM & MLP\n",
    "\n",
    "combinedInput = concatenate([mlp.output, lstm.output])\n",
    "x = Dense(1, activation=\"sigmoid\")(combinedInput)\n",
    "\"\"\"\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"linear\")(x)\"\"\"\n",
    "model = Model(inputs=[mlp.input, lstm.input], outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950649f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313471f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=[ X_train_tf.toarray(),X_train_fin], y=y_train,validation_data = [[X_test_tf.toarray(), X_test_fin],y_test],use_multiprocessing=True ,epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849ca901",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).to_csv(\"mlp&lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "\n",
    "print(history.history.keys())\n",
    "Fig, ax =plt.subplots(1,2,figsize=(10,3))\n",
    "\n",
    "# summarize history for accuracy\n",
    "\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_title('model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('model loss')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"proposed\",dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x=[ X_test_tf.toarray(),X_test_fin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb117a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, [round(i[0]) for i in pred],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1a50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
